{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYU-SP21-Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJi2ZhsU2DpBWLsQd+dyUc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyEHamlin/NYU-DL-SP20/blob/main/NYU_SP21_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Deep Learning\n",
        "* General Page (2021 most current, notated) https://cds.nyu.edu/deep-learning/\n",
        "* 2020\n",
        "    * Youtube List: https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq\n",
        "    * Class Site: https://atcold.github.io/pytorch-Deep-Learning/\n",
        "    * Google Drive: https://bitly.com/DLSP20\n",
        "    * Github: https://github.com/Atcold/pytorch-Deep-Learning\n",
        "\n",
        "* 2021\n",
        "    * Youtube: https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI\n",
        "    * Github: https://github.com/Atcold/NYU-DLSP21\n"
      ],
      "metadata": {
        "id": "ps0qz0RpH14E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01 (see 2020 Notebook)"
      ],
      "metadata": {
        "id": "--w6balvHsSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 01L: Back Propagation: https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2\n",
        "# Three Paradimes: https://youtu.be/nTlCqaL7fCY?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&t=82\n",
        "* Supervised Learning\n",
        "* Reinforced Learning\n",
        "* Unsupervised Learning\n",
        "    * DL has replaced hand-done learning\n",
        "\n",
        "# Parameterized Model (e.g. mutimentional array):  $\\bar{y} = G(x,w)$\n",
        "* e.g. Linear Regression: $\\bar{y} = \\sum\\limits_{i}w_i x_i  \\hspace{+1em} C(y,\\bar{y}) = ||y-\\bar{y}||^2$\n",
        "* e.g. Nearest Neighbor: $\\bar{y} = argmin_k ||x-w_k,.||^2$\n",
        "\n",
        "# Block Diagram Notation: \n",
        "![Notation](https://raw.githubusercontent.com/RoyEHamlin/NYU-DL-SP20/main/01L-0.07.46-Notation-Convention.PNG \"Notation\") \n",
        "# Loss Function, avg loss\n",
        "* simple per-sample loss function: $L(x,y,w) = C(y,G(x,w))$\n",
        "* set of samples: $S = \\{(x[p],y[p]) \\hspace{+0.5em} / \\hspace{+0.5em} p = 0...P-1\\}$\n",
        "* avg. loss over the set: $\\mathcal{L}(S,w) = \\frac{1}{P} \\sum\\limits_{(x,y)} L(x,y,w) = \\frac{1}{P}L(x[p],y[p],w)$ (i.e. <u>loss function</u>)\n",
        "* see: https://youtu.be/nTlCqaL7fCY?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&t=547\n",
        "\n",
        "# [00:12:23](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=743s) – Gradient descent \n",
        "* following the most steep 'local' direction\n",
        "* Each sample is a noisy 'estimate'\n",
        "* Gradient Decent\n",
        "    * Full (Batch) gradient: $w \\gets w - \\eta \\frac{\\partial\\mathcal{L}(S,w)}{\\partial w}$\n",
        "    * Stochastic Gradient Decent (SGD) (i.e. random sample): $w \\gets w - \\eta \\frac{\\partial\\mathcal{L}(x[p],y[p],w)}{\\partial w}$ (a noisy estimate)\n",
        "* SGD exploits redundncy in the samples\n",
        "    * usually faster than full gradient\n",
        "    * in practice, use mini-batches in parallel. (batching exclusively for hardware advantages) \n",
        "* NOTE: \"Typically\", there are not many local minima to cause issues with the above functions.\n",
        "* LATER: Stay away from local minima, will circle back in a later lecture\n",
        "* Most traditional way of 'shuffling data' Training set, shuffle in random order then run through them. \n",
        "* Batch Size: determined by hardware.\n",
        "* (-) Generally, if class size = K, then if batch 2K, then waisting computation (not ness time).\n",
        "\n",
        "\n",
        "# [00:30:47](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=1847s) – Traditional neural nets \n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-0.25.56-Traditional-NN.PNG?raw=true)\n",
        "* ReLu: 0 or 1 selector\n",
        "    * Typically, every element of later x is connected to each element of x2\n",
        "    * Stacked linear and non-linear functional blocks: \n",
        "        * $s[i]=\\sum\\limits_{j\\in UP(i)} w[i,j] \\bullet z[j]$\n",
        "            * w[i,j] is the weight connecting i and j.\n",
        "        * activation $z[i] =f(s[i])$\n",
        "    * NOTE: graph must be asyncronous (i.e. cannot have a loop)\n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-0.33.53-Stacked-blocks.PNG?raw=true)\n",
        "\n",
        "# [00:35:07](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=2107s) – Backprop through a non-linear function \n",
        "* Chain Rule: \n",
        "    * 'wiggling' of 'z' impacts and 'wiggles' 'C' (C / z = partial derivative)\n",
        "* Perturbations: \n",
        "\n",
        "# [00:40:41](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=2441s) – Backprop through a weighted sum \n",
        "## Perturbations: \n",
        "![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-0.41.43-Weighted-Sum-Perturbaations.PNG?raw=true)\n",
        "## Block Diagrams\n",
        "![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-0.50.28-Block-Diag-of-TNN.PNG?raw=true)\n",
        "\n",
        "# [00:50:55](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=3055s) – PyTorch implementation \n",
        "![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-0.51.01-OOP-Python-of-Block-Diagram.PNG?raw=true)\n",
        "    * m0 thru m2: 'module'\n",
        "    * lower case lettering: (e.g. 'relu') just a function w/o internal parameters\n",
        "    * Capital Lettering: (e.g. nn.Linear) contains internal parameters, etc.\n",
        "\n",
        "# [00:57:18](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=3438s) – Backprop through a functional module \n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-1.01.03-Jacobian-matrix.PNG?raw=true)\n",
        "* $\\partial c$ =  gradient vector (however, we typically write as a 'horizontal row' vector.  Same size as $d_g$)\n",
        "* $\\partial z_f$ = matrix (derivative of one vector w/ respect to another vector)\n",
        "    * rows = size of $d_g$\n",
        "    * col = size of $z_f$\n",
        "    * 'i'th output of $Z_g$\n",
        "    * 'j'th component of $Z_f$\n",
        "* jacobian matrix: matrix of weights ???\n",
        "\n",
        "# [01:05:08](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=3908s) – Backprop through a functional module \n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-1.07.06-Backpropagation-Block-in-Pytorch.PNG?raw=true)\n",
        "* Matrix Dimentions:\n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-1.09.54-MxN-dimentional-Matrix.PNG?raw=true)\n",
        "* Basic Modules\n",
        "    * Linear\n",
        "    * ReLU\n",
        "    * Duplicate\n",
        "    * Add\n",
        "    * Max\n",
        "    * LogSoftMax\n",
        "* ![](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/01L-1.10.44-Basic-Modules.PNG?raw=true)\n",
        "* Non-Linear & Loss functions in PyTorch\n",
        "    * Relu, sigmoids, variations\n",
        "    * squared error, cross-entropy, hinge, ranking loss, variants\n",
        "* DAG is OK for backpropagation (provided no loops, a 'partial order')\n",
        "    * If DAG has loops, we need to 'unroll' them\n",
        "    * e.g. recurrent NN (RNN) and backprop through time\n",
        "# [01:12:15](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=4335s) – Backprop in practice \n",
        "* Examples\n",
        "    * Use ReLU non-linearities (tan h aand logistic are falling out of favor (-))\n",
        "    * Proper weight initialization \n",
        "***** https://youtu.be/nTlCqaL7fCY?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&t=4363 ****\n",
        "\n",
        "# [01:33:15](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=5595s) – Learning representations \n",
        "\n",
        "# [01:42:14](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=6134s) – Shallow networks are universal approximators! \n",
        "\n",
        "# [01:47:25](https://www.youtube.com/watch?v=nTlCqaL7fCY&list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&index=2&t=6445s) – Multilayer architectures == compositional structure of data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KJlRQafTe3PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Co76uXOSkcHQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}