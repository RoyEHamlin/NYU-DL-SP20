{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYU-SP20-Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyxky9lkBexCHJ/uTz/Xcj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoyEHamlin/NYU-DL-SP20/blob/main/NYU_SP20_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Deep Learning\n",
        "* Youtube List: https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq\n",
        "* Class Site: https://atcold.github.io/pytorch-Deep-Learning/\n",
        "* Google Drive: https://bitly.com/DLSP20\n",
        "* Github: https://github.com/Atcold/pytorch-Deep-Learning\n"
      ],
      "metadata": {
        "id": "ps0qz0RpH14E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 01"
      ],
      "metadata": {
        "id": "--w6balvHsSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wk 01: Lecture: History, motivation, and evolution of Deep Learning\n",
        "https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1\n",
        "> ## Course \n",
        "\n",
        "\n",
        "* Part A: We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.\n",
        "  * [0:03:37](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=217s) ‚Äì Inspiration of Deep Learning and Its History, Supervised Learning \n",
        "    * ![Plan 01](https://raw.githubusercontent.com/RoyEHamlin/NYU-DL-SP20/main/plan-01.png \"Plan 1\") \n",
        "    * ![Plan 02](https://raw.githubusercontent.com/RoyEHamlin/NYU-DL-SP20/main/plan-02.png \"Plan 2\") \n",
        "    * ![Plan 03](https://raw.githubusercontent.com/RoyEHamlin/NYU-DL-SP20/main/plan-03.png \"Plan 3\") \n",
        "  * [0:24:21](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=1461s) ‚Äì History of Pattern Recognition and Introduction to Gradient Descent \n",
        "    * <u>Perceptron</u>: Weights are motorized potentionmeters\n",
        "    * <u>Adaline</u>: Weights are electrochemical \"memistors\"\n",
        "    * Models\n",
        "        * Standard Pattern Recog.: Feature Extractor(hand engineered) ‚û°Ô∏è Trainable Classifier(Trainable)\n",
        "        * Deep Learning: Low/mid/high features classifier (all trainable)\n",
        "        * (Deep) Multi-Layer NN https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=1885\n",
        "            1. Mult lyers of SIMPLE UNITS\n",
        "            2. each unit computes a WEIGHTED SUM of inputs\n",
        "            3. weighted sum pass through a NON-LINEAR function\n",
        "            4. learning algo changes WEIGHTS\n",
        "    * supervised ML = Function Optimization https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=2023\n",
        "        * downhill fog e.g.\n",
        "        * each sample is 'fuzzy', so small amt. of randomization in path.\n",
        "        * SGD: Stochstic Grdient Decent\n",
        "        * SGD faster than training on full data set\n",
        "  * [0:38:56](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=2336s) ‚Äì Computing Gradients by Backpropagation, Hierarchical Representation of the Visual Cortex\n",
        "    * Hubel & Wiesel: https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=2580\n",
        "        * <u>Simple cells</u>: detect local features\n",
        "        * <u>Complex cells</u>:  convolution/pool the outputs of simple cells w/i a retinotopic neighborhood. Allows for some shifting of the 'edges' while still 'lighting up' the same areas of the brain (i.e. we recognize what we see, regardless of orientation).\n",
        "\n",
        "\n",
        "* Part B: We first discuss the evolution of CNNs, from Fukushima to LeCun to Alexnet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.\n",
        "  * [0:49:25](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=2965s) ‚Äì Evolution of CNNs \n",
        "    * NN Hardware: https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=3595\n",
        "    * Error Rate: ![dec. error rate](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/Wk01-dec.%20error%20rate.png?raw=true \"Error Rate\") \n",
        "    * ![diminishing returns for more layers](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/Wk01-opt%20rate.png?raw=true \"diminishing returns for more layers\") \n",
        "  * [1:05:55](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=3955s) ‚Äì Deep Learning & Feature Extraction \n",
        "    * Multilayer arch. == compositional structure of data. (Compositionality)\n",
        "    * Natural Data is compositional ‚Üí efficiently representable hierarchy\n",
        "    * Pyramid Network Examples\n",
        "        * ![Pyramid Networks](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/Wk01-Pyramid-Network.png?raw=true \"Pyramid Networks\") \n",
        "        * e.g. Mask RCNN (masking each region of interest\n",
        "            * ![Mask RCNN e.g.](https://github.com/RoyEHamlin/NYU-DL-SP20/blob/main/Wk01-Mask-RCNN.png?raw=true \"Mask RCNN e.g.\") \n",
        "        * RetinaNet/FPN (one pass obj. detection)\n",
        "    * <u>AEBS</u>: Advanced Emergency Braking System\n",
        "    * e.g. Medical Imaging: MRI https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=4370\n",
        "        * Looks at entire 3D image, not as segments\n",
        "    * Why does it work so well\n",
        "        * We can aprox any function w/ 2 layers:  Why layers?\n",
        "        * Why CNN so special?  Why do they work well on natural signals?\n",
        "        * The obj. funct. are highly non-convex: Why doesn't SGD get trapped in local minima? ‚ùì\n",
        "        * The networks are widely over-parameterized:  Why do they not fit?\n",
        "            * Why do NN break almost every Statistical norms (e.g. N+ parameters ‚Üí(converge to)‚Üí N parameters)\n",
        "    * can and can't\n",
        "        * can\n",
        "            1. autonomous cars\n",
        "            2. med. imaging\n",
        "            3. personalized medicine\n",
        "            4. adequate lang. translation\n",
        "            5. useful & stupid chatbots\n",
        "            6. information search, retrieval, filtering üß†\n",
        "            7. applcations in energy, finance, manufact, environm protection, commerce, law, artistic creation, games\n",
        "        * can't (yet)\n",
        "            1. Machines w/ common sense\n",
        "            2. intelligent personal assistants\n",
        "            3. \"smart\" chatbots\n",
        "            4. household robots\n",
        "            5. agile & dexterous robots\n",
        "            6. AGI: Artificial General Intelligence\n",
        "        * representation: raw data ‚Üí useful data (expanding dim. of representation so that things are more likely to become <u>linearly separable</u> https://youtu.be/0bMe_vCZo30?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&t=4657\n",
        "            * Space Tiling\n",
        "            * random projections\n",
        "            * polynomial classifier (feature cross-products)\n",
        "            * radial basis functions\n",
        "            * kernel machines\n",
        "  * [1:19:27](https://www.youtube.com/watch?v=0bMe_vCZo30&list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq&index=1&t=4767s) ‚Äì Learning Representations\n",
        "\n"
      ],
      "metadata": {
        "id": "UxaXRZTgfgpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wk 01: Motivation of Deep Learning, and Its History and Inspiration\n",
        "https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-1/\n",
        "### Supervised Learning\n",
        "#### Perceptron\n",
        "* 2 layer\n",
        "* 1st: fixed weights\n",
        "* 2nd: trainable\n",
        "\n",
        "#### Adaline (cont. pred. values to 'scale' updating feedback.)\n",
        "#### Pattern Recognition & Gradient Decent\n",
        "* before deep learning\n",
        "  * feature extraction\n",
        "  * vector of features then fed to trainable classifier\n",
        "* After deep learning\n",
        "  * 2-stage -> seq. of modules\n",
        "  * modules have tunable paramenters & nonlinearity\n",
        "* e.g. \n",
        "  * ReLU: non-linear activation function that returns the max value between 0 and input variable 'x'.\n",
        "    * f(x) = max(0,x) \n",
        "    * (+) helps save computational time during training (compared to using other activation functions such as sigmoid, logistic, etc.)\n",
        "    * https://deepai.org/machine-learning-glossary-and-terms/relu\n",
        "  * NN = weighted sum of components x rows in matrix\n",
        "* Goal of Sup. learning\n",
        "  * paramenter values = minimize the avg. distance/penalty/divergence btwn target & result\n",
        "  * aka __computational gradient__\n",
        "    * SGD : Stochastic Gradient Decent\n",
        "      * (+) fast converging model on very large training set\n",
        "      * (+) better overall generalization \n",
        "#### Gradient by Backpropagation\n",
        "* takes vector (multi-dimentional), NOT scalar inputs\n",
        "* applies to multiple layers\n",
        "* NOTE: assumptions on matrix help to reduce large matrices (e.g. 256x256 = 200,000 valued matrix)\n",
        "#### Visul recognition done hierarchically \n",
        "* Visual process = feet forward model\n"
      ],
      "metadata": {
        "id": "9rnrJcAyebFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wk 01: Evolution and Uses of CNNs and Why Deep Learning\n",
        "https://atcold.github.io/pytorch-Deep-Learning/en/week01/01-2/"
      ],
      "metadata": {
        "id": "nuKGunCeaACG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evolution of CNNS\n",
        "### Deep Learning and Feature Extraction\n",
        "> multilayer networkes work well b/c use compositional structure of _natural_ data (i.e. hierarchical) \n",
        "> ### Mask RCNNs: creating masks for objects w/i an image\n",
        "* creating masks for each obj. in an image\n",
        "* <u>instance segmentation:</u> # of instances of an obj. in an image.\n",
        "> ### CNN uses \n",
        "* autonomous driving\n",
        "* nalysis of medical images\n",
        "> ### Unansered questions\n",
        "* why multi lyers perform better?\n",
        "* why do CNN do well w/ __natural__ data sets\n",
        "  * speech\n",
        "  * images\n",
        "  * text\n",
        "* how do non-convex optimize so well?\n",
        "* Why do over-parametrised architectures work? \n",
        "> ### common feature extraction approaches\n",
        "* space tilting\n",
        "* random projections\n",
        "* polynomial classiers (feature cross-products)\n",
        "* radial bias functions\n",
        "* kernel machines\n",
        "### Learning representations\n",
        "> ### What is \"deep\"?\n",
        "* deep ‚â† 2 layers\n",
        "* deep ‚â† every layer sees all raw features\n",
        "* deep = multi layers used to build a <u>hierarchy of features of inc. complexity</u>\n"
      ],
      "metadata": {
        "id": "iNF_dLZHaG9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wekk 02 "
      ],
      "metadata": {
        "id": "KJlRQafTe3PO"
      }
    }
  ]
}